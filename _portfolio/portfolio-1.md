---
title: "PotterGPT: Character-level Transformer"
excerpt: "We develop a minimal character-level transformer model built from scratch in PyTorch. The model demonstrates core components of GPT-style architectures including token embedding, self-attention, positional encoding, and autoregressive training. A simple bigram dataset is used to illustrate text generation capabilities."
We develop a minimal character-level transformer model entirely from scratch using PyTorch. The model architecture reflects the core principles of modern transformer-based language models and includes:
- Learnable token embeddings
- Multi-head self-attention layers
- Positional encoding
- Layer normalization and residual connections
- Autoregressive text generation training
This implementation serves both as a practical introduction to transformer internals and as a baseline model for experimentation. It is trained on a simple bigram dataset to illustrate sequential prediction and text synthesis from scratch.

img:  image (1).png
collection: portfolio
date: 2025-06-04
---

<img src="/images/image (1).png" width="500" align=right style="margin-left: 10px; margin-top: 5px;">

<span>
We develop a minimal character-level transformer model entirely from scratch using PyTorch. The model architecture reflects the core principles of modern transformer-based language models and includes:

- Learnable token embeddings
- Multi-head self-attention layers
- Positional encoding
- Layer normalization and residual connections
- Autoregressive text generation training

This implementation serves both as a practical introduction to transformer internals and as a baseline model for experimentation. It is trained on a simple bigram dataset to illustrate sequential prediction and text synthesis from scratch.

The full implementation and training scripts are provided in the repository.

<b>GitHub Repository:</b> <a href="https://github.com/JigyanshuPati/Character-level-Transformer" target="_blank">Character-level Transformer</a>
</span>
